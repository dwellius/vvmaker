#!/usr/bin/env python3
import os
import json
import random
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import time
import math

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã
def install_requirements():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"""
    packages = [
        'faster-whisper',
        'scenedetect[opencv]',
        'openai',
        'nltk',
        'textstat',
        'colorama',
        'tqdm'
    ]
    
    for package in packages:
        try:
            __import__(package.replace('-', '_').split('[')[0])
        except ImportError:
            print(f"–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
try:
    install_requirements()
except:
    print("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞–∫–µ—Ç—ã –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–∏–ª–∏—Å—å, –ø—Ä–æ–¥–æ–ª–∂–∞—é —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é...")

from faster_whisper import WhisperModel
try:
    from scenedetect import detect, ContentDetector
    SCENEDETECT_NEW = True
except ImportError:
    try:
        from scenedetect import VideoManager, SceneManager
        from scenedetect.detectors import ContentDetector
        SCENEDETECT_NEW = False
    except ImportError:
        print("‚ö†Ô∏è Scenedetect –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è –Ω–∞—Ä–µ–∑–∫–∞")
        SCENEDETECT_NEW = None

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è OpenAI –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

try:
    import nltk
    import textstat
    ANALYSIS_AVAILABLE = True
except ImportError:
    ANALYSIS_AVAILABLE = False
    print("‚ö†Ô∏è –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã")

from colorama import init, Fore, Style, Back
from tqdm import tqdm

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
init(autoreset=True)

class VideoAnalyzer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ —Å fallback —Ä–µ–∂–∏–º–∞–º–∏"""
    
    def __init__(self, config_file="config.json"):
        self.config = self.load_config(config_file)
        if ANALYSIS_AVAILABLE:
            self.setup_nltk()
        
    def setup_nltk(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            try:
                print("–ó–∞–≥—Ä—É–∂–∞—é —è–∑—ã–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
                nltk.download('punkt', quiet=True)
                nltk.download('stopwords', quiet=True)
                nltk.download('vader_lexicon', quiet=True)
            except:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å NLTK –¥–∞–Ω–Ω—ã–µ, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–∏—Ö")
    
    def load_config(self, config_file):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        default_config = {
            "whisper_model": "small",
            "scene_threshold": 30.0,
            "min_clip_duration": 10,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ
            "max_clip_duration": 60,
            "min_words": 5,  # –£–º–µ–Ω—å—à–µ–Ω–æ
            "segment_duration": 30,  # –î–ª—è —Å–ª—É—á–∞—è –∫–æ–≥–¥–∞ —Å—Ü–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
            "output_resolution": "1080x1920",
            "subtitle_styles": {
                "modern": {
                    "font": "Arial Black",
                    "size": 56,
                    "color": "&H00FFFFFF",
                    "outline": 4,
                    "shadow": 2
                }
            },
            "emoji_map": {
                "—Å–º–µ—à–Ω–æ": "üòÇ", "—é–º–æ—Ä": "üòÑ", "—Ä–∂–∞–∫–∞": "ü§£",
                "–æ—à–∏–±–∫–∞": "‚ùå", "–ø—Ä–æ–±–ª–µ–º–∞": "‚ö†Ô∏è", "–±–∞–≥": "üêõ",
                "–≤–∞–∂–Ω–æ": "‚≠ê", "–≤–Ω–∏–º–∞–Ω–∏–µ": "üì¢", "–∫–ª—é—á–µ–≤–æ–µ": "üîë",
                "–∏–¥–µ—è": "üí°", "–º—ã—Å–ª—å": "üß†", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è": "üöÄ",
                "—Å–æ–≤–µ—Ç": "üìå", "–ª–∞–π—Ñ—Ö–∞–∫": "üíØ", "—Å–µ–∫—Ä–µ—Ç": "üîê",
                "–¥–µ–Ω—å–≥–∏": "üí∞", "–±–∏–∑–Ω–µ—Å": "üìà", "—É—Å–ø–µ—Ö": "üéØ",
                "–ª—é–±–æ–≤—å": "‚ù§Ô∏è", "—Å–µ–º—å—è": "üë®‚Äçüë©‚Äçüëß‚Äçüë¶", "–¥—Ä—É–∑—å—è": "üë•",
                "—Å–ø–æ—Ä—Ç": "‚öΩ", "–∑–¥–æ—Ä–æ–≤—å–µ": "üí™", "—Ñ–∏—Ç–Ω–µ—Å": "üèãÔ∏è",
                "–µ–¥–∞": "üçï", "–≥–æ—Ç–æ–≤–∫–∞": "üë®‚Äçüç≥", "—Ä–µ—Ü–µ–ø—Ç": "üìù",
                "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ": "‚úàÔ∏è", "–ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ": "üó∫Ô∏è", "–æ—Ç–¥—ã—Ö": "üèñÔ∏è",
                "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "üíª", "AI": "ü§ñ", "–±—É–¥—É—â–µ–µ": "üîÆ",
                "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ": "üìö", "—É—á–µ–±–∞": "üéì", "–∑–Ω–∞–Ω–∏—è": "üìñ"
            },
            "keywords": {
                "high_priority": ["–≤–∞–∂–Ω–æ", "—Å–µ–∫—Ä–µ—Ç", "–æ—à–∏–±–∫–∞", "—Å–æ–≤–µ—Ç", "–ª–∞–π—Ñ—Ö–∞–∫", "–≤–Ω–∏–º–∞–Ω–∏–µ"],
                "medium_priority": ["–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—Å–º–µ—à–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ"],
                "emotions": ["—Ä–∞–¥–æ—Å—Ç—å", "–≥—Ä—É—Å—Ç—å", "–∑–ª–æ—Å—Ç—å", "—É–¥–∏–≤–ª–µ–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö"],
                "engagement": ["–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å", "–ª–∞–π–∫", "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "–ø–æ–¥–µ–ª–∏—Ç–µ—Å—å"]
            }
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except:
                print("‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏")
        else:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
                
        return default_config

class EnhancedTranscriber:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
    
    def __init__(self, model_size="small"):
        self.model = WhisperModel(model_size, device="cpu")
        print(f"{Fore.CYAN}üé§ Whisper –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_size}")
        
    def transcribe_with_analysis(self, video_path: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        print(f"{Fore.YELLOW}üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –≤–∏–¥–µ–æ: {video_path}")
        
        segments, info = self.model.transcribe(
            video_path, 
            word_timestamps=True
        )
        
        transcript = []
        full_text = ""
        
        for seg in tqdm(segments, desc="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–µ–≥–º–µ–Ω—Ç—ã"):
            words = []
            if seg.words:
                for w in seg.words:
                    words.append({
                        "start": w.start,
                        "end": w.end,
                        "text": w.word.strip(),
                        "confidence": getattr(w, 'probability', 1.0)
                    })
            
            segment_text = seg.text.strip()
            full_text += segment_text + " "
            
            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞
            segment_data = {
                "start": seg.start,
                "end": seg.end,
                "text": segment_text,
                "words": words,
                "word_count": len(segment_text.split()),
                "excitement_score": self.calculate_excitement(segment_text),
                "has_question": "?" in segment_text,
                "has_exclamation": "!" in segment_text
            }
            
            transcript.append(segment_data)
        
        return {
            "segments": transcript,
            "full_text": full_text.strip(),
            "total_duration": info.duration,
            "language": getattr(info, 'language', 'ru'),
            "language_probability": getattr(info, 'language_probability', 1.0)
        }
    
    def calculate_excitement(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å '–≤–æ–∑–±—É–∂–¥–µ–Ω–∏—è' —Ç–µ–∫—Å—Ç–∞"""
        excitement_words = [
            "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏", "–ø–æ—Ç—Ä—è—Å–∞—é—â–µ",
            "—à–æ–∫", "–≤–∞—É", "–æ—Ñ–∏–≥–µ—Ç—å", "–∫—Ä—É—Ç–æ", "—Å—É–ø–µ—Ä", "–º–µ–≥–∞", "–∫–ª–∞—Å—Å–Ω–æ"
        ]
        
        score = 0
        words = text.lower().split()
        
        for word in words:
            if any(ew in word for ew in excitement_words):
                score += 2
            if word.endswith("!") or "!" in text:
                score += 1
                
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        return min(score / max(len(words), 1), 1.0)

class AIContentAnalyzer:
    """AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å fallback —Ä–µ–∂–∏–º–æ–º"""
    
    def __init__(self):
        self.ai_available = False
        if OPENAI_AVAILABLE:
            try:
                self.client = openai.OpenAI(
                    api_key="sk-emergent-fDbDaD9E72a58Ab4bE",
                    base_url="https://api.emergent.com"
                )
                self.ai_available = True
            except:
                print("‚ö†Ô∏è AI –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
    
    def analyze_segments(self, transcript_data: Dict) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        segments = transcript_data["segments"]
        
        print(f"{Fore.MAGENTA}ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–Ω—Ç...")
        
        analyzed_segments = []
        
        # –ü—Ä–æ–±—É–µ–º AI –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
        if self.ai_available:
            try:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=1,
                    timeout=10
                )
                print(f"{Fore.GREEN}‚úÖ AI –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return self.ai_analyze_segments(segments)
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({str(e)[:50]}...), –∏—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        return self.local_analyze_segments(segments)
    
    def ai_analyze_segments(self, segments):
        """AI –∞–Ω–∞–ª–∏–∑ (–∫–æ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)"""
        analyzed_segments = []
        
        for i in range(0, len(segments), 5):
            batch = segments[i:i+5]
            batch_text = "\n".join([f"{j+1}. {seg['text']}" for j, seg in enumerate(batch)])
            
            if len(batch_text.strip()) < 20:
                for seg in batch:
                    seg["ai_score"] = 0.3
                    seg["ai_reason"] = "–ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"
                analyzed_segments.extend(batch)
                continue
            
            try:
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": """–û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç –æ—Ç 0.0 –¥–æ 1.0 –ø–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –≤–∏–¥–µ–æ.
                            –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON: {"segments": [{"index": 1, "score": 0.8, "reason": "–ø—Ä–∏—á–∏–Ω–∞"}]}"""
                        },
                        {
                            "role": "user",
                            "content": f"–û—Ü–µ–Ω–∏ —ç—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç—ã:\n{batch_text}"
                        }
                    ],
                    temperature=0.3,
                    max_tokens=300,
                    timeout=15
                )
                
                result = json.loads(response.choices[0].message.content)
                
                for item in result.get("segments", []):
                    idx = item.get("index", 1) - 1
                    if idx < len(batch):
                        batch[idx]["ai_score"] = item.get("score", 0.5)
                        batch[idx]["ai_reason"] = item.get("reason", "AI –∞–Ω–∞–ª–∏–∑")
                        
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                for seg in batch:
                    score = self.local_analysis(seg)
                    seg["ai_score"] = score
                    seg["ai_reason"] = "–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
            
            analyzed_segments.extend(batch)
            time.sleep(0.1)
        
        return analyzed_segments
    
    def local_analyze_segments(self, segments):
        """–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI"""
        print(f"{Fore.CYAN}üîç –ò—Å–ø–æ–ª—å–∑—É—é –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
        
        for segment in tqdm(segments, desc="–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª–æ–∫–∞–ª—å–Ω–æ"):
            score = self.local_analysis(segment)
            segment["ai_score"] = score
            segment["ai_reason"] = "–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
        
        return segments
    
    def local_analysis(self, segment: Dict) -> float:
        """–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        score = 0.0
        text = segment["text"].lower()
        
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        high_value_words = ["—Å–µ–∫—Ä–µ—Ç", "–æ—à–∏–±–∫–∞", "—Å–æ–≤–µ—Ç", "–≤–∞–∂–Ω–æ", "–ª–∞–π—Ñ—Ö–∞–∫", "–≤–Ω–∏–º–∞–Ω–∏–µ"]
        emotion_words = ["—Å–º–µ—à–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "—à–æ–∫", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ", "–∫—Ä—É—Ç–æ", "—Å—É–ø–µ—Ä"]
        
        for word in high_value_words:
            if word in text:
                score += 0.3
                
        for word in emotion_words:
            if word in text:
                score += 0.25
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–ª–ª—ã –∑–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        if "?" in text:
            score += 0.15
        if "!" in text:
            score += 0.2
            
        # –£—á–∏—Ç—ã–≤–∞–µ–º excitement_score
        score += segment.get("excitement_score", 0) * 0.3
        
        # –î–ª–∏–Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ (–Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π)
        word_count = segment.get("word_count", 0)
        if 8 <= word_count <= 25:
            score += 0.1
        
        return min(score, 1.0)

class ImprovedSceneDetector:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å—Ü–µ–Ω —Å fallback —Ä–µ–∂–∏–º–æ–º"""
    
    def __init__(self, threshold=30.0):
        self.threshold = threshold
        
    def detect_scenes(self, video_path: str, transcript_data: Dict) -> List[Tuple]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ü–µ–Ω —Å fallback –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
        print(f"{Fore.GREEN}üé¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ü–µ–Ω—ã –≤ –≤–∏–¥–µ–æ...")
        
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å scene detection
        if SCENEDETECT_NEW is not None:
            scenes = self.detect_with_scenedetect(video_path)
            if scenes:
                print(f"{Fore.GREEN}‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ –¥–µ—Ç–µ–∫—Ç–æ—Ä: {len(scenes)}")
                return scenes
        
        # –ï—Å–ª–∏ —Å—Ü–µ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º –∏—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        print(f"{Fore.YELLOW}‚ö†Ô∏è –î–µ—Ç–µ–∫—Ç–æ—Ä —Å—Ü–µ–Ω –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, —Å–æ–∑–¥–∞—é —Å—Ü–µ–Ω—ã –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏")
        scenes = self.create_scenes_from_transcript(transcript_data)
        print(f"{Fore.GREEN}‚úÖ –°–æ–∑–¥–∞–Ω–æ —Å—Ü–µ–Ω –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {len(scenes)}")
        return scenes
    
    def detect_with_scenedetect(self, video_path: str):
        """–î–µ—Ç–µ–∫—Ü–∏—è —Å—Ü–µ–Ω —á–µ—Ä–µ–∑ scenedetect"""
        try:
            if SCENEDETECT_NEW:
                # –ù–æ–≤—ã–π API
                scene_list = detect(video_path, ContentDetector(threshold=self.threshold))
            else:
                # –°—Ç–∞—Ä—ã–π API  
                video_manager = VideoManager([video_path])
                scene_manager = SceneManager()
                scene_manager.add_detector(ContentDetector(threshold=self.threshold))
                video_manager.start()
                scene_manager.detect_scenes(frame_source=video_manager)
                scene_list = scene_manager.get_scene_list()
            
            scenes = []
            for i, (start, end) in enumerate(scene_list):
                if SCENEDETECT_NEW:
                    start_sec = start.get_seconds()
                    end_sec = end.get_seconds()
                    start_tc = start.get_timecode()
                    end_tc = end.get_timecode()
                else:
                    start_sec = start.get_seconds()
                    end_sec = end.get_seconds() 
                    start_tc = start.get_timecode()
                    end_tc = end.get_timecode()
                
                duration = end_sec - start_sec
                scenes.append({
                    "start_timecode": start_tc,
                    "end_timecode": end_tc,
                    "start_seconds": start_sec,
                    "end_seconds": end_sec,
                    "duration": duration,
                    "scene_id": i + 1
                })
            
            return scenes if scenes else None
            
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å—Ü–µ–Ω: {e}")
            return None
    
    def create_scenes_from_transcript(self, transcript_data: Dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏"""
        segments = transcript_data["segments"]
        total_duration = transcript_data.get("total_duration", 0)
        
        if not segments:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –¥–µ–ª–∏–º –≤–∏–¥–µ–æ –Ω–∞ —Ä–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏
            return self.create_equal_scenes(total_duration)
        
        scenes = []
        current_scene_start = 0
        current_scene_duration = 0
        target_duration = 30  # –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ —Å—Ü–µ–Ω—ã
        
        for i, segment in enumerate(segments):
            segment_duration = segment["end"] - segment["start"]
            current_scene_duration += segment_duration
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ü–µ–Ω—É –µ—Å–ª–∏:
            # 1. –î–æ—Å—Ç–∏–≥–ª–∏ —Ü–µ–ª–µ–≤–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            # 2. –≠—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
            if current_scene_duration >= target_duration or i == len(segments) - 1:
                scene_end = segment["end"]
                
                if current_scene_duration >= 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ü–µ–Ω—ã
                    scenes.append({
                        "start_timecode": self.seconds_to_timecode(current_scene_start),
                        "end_timecode": self.seconds_to_timecode(scene_end),
                        "start_seconds": current_scene_start,
                        "end_seconds": scene_end,
                        "duration": current_scene_duration,
                        "scene_id": len(scenes) + 1
                    })
                
                current_scene_start = scene_end
                current_scene_duration = 0
        
        return scenes if scenes else self.create_equal_scenes(total_duration)
    
    def create_equal_scenes(self, total_duration):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–≤–Ω—ã—Ö —Å—Ü–µ–Ω –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        if total_duration <= 0:
            total_duration = 60  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–∏–Ω—É—Ç—É
        
        scene_duration = 30  # 30-—Å–µ–∫—É–Ω–¥–Ω—ã–µ —Å—Ü–µ–Ω—ã
        scenes = []
        
        start = 0
        scene_id = 1
        
        while start < total_duration:
            end = min(start + scene_duration, total_duration)
            
            scenes.append({
                "start_timecode": self.seconds_to_timecode(start),
                "end_timecode": self.seconds_to_timecode(end),
                "start_seconds": start,
                "end_seconds": end,
                "duration": end - start,
                "scene_id": scene_id
            })
            
            start = end
            scene_id += 1
        
        return scenes
    
    def seconds_to_timecode(self, seconds):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–µ–∫—É–Ω–¥ –≤ —Ç–∞–π–º–∫–æ–¥"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds - int(seconds)) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}.{millisecs:03d}"

class SimpleVideoProcessor:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤–∏–¥–µ–æ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏"""
    
    def __init__(self, config):
        self.config = config
        
    def create_clips(self, video_path, analyzed_segments, scenes, output_dir="enhanced_clips"):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–ø–æ–≤"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"{Fore.CYAN}üé• –°–æ–∑–¥–∞—é –∫–ª–∏–ø—ã...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
        good_segments = [s for s in analyzed_segments if s.get("ai_score", 0) > 0.3]
        if not good_segments:
            good_segments = analyzed_segments[:5]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ
            
        good_segments.sort(key=lambda x: x.get("ai_score", 0), reverse=True)
        
        created_clips = []
        
        for i, segment in enumerate(tqdm(good_segments[:min(10, len(good_segments))], desc="–°–æ–∑–¥–∞—é –∫–ª–∏–ø—ã")):
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ü–µ–Ω—É
            scene = self.find_best_scene(segment, scenes)
            if not scene:
                # –°–æ–∑–¥–∞–µ–º —Å—Ü–µ–Ω—É –≤–æ–∫—Ä—É–≥ —Å–µ–≥–º–µ–Ω—Ç–∞
                scene = self.create_scene_around_segment(segment)
                
            clip_info = self.create_single_clip(
                video_path, segment, scene, i + 1, output_dir
            )
            
            if clip_info:
                created_clips.append(clip_info)
        
        return created_clips
    
    def find_best_scene(self, segment, scenes):
        """–ù–∞–π—Ç–∏ –ª—É—á—à—É—é —Å—Ü–µ–Ω—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        seg_start, seg_end = segment["start"], segment["end"]
        
        for scene in scenes:
            if (scene["start_seconds"] <= seg_start and 
                scene["end_seconds"] >= seg_end and
                scene["duration"] >= self.config["min_clip_duration"] and
                scene["duration"] <= self.config["max_clip_duration"]):
                return scene
        
        return None
    
    def create_scene_around_segment(self, segment):
        """–°–æ–∑–¥–∞—Ç—å —Å—Ü–µ–Ω—É –≤–æ–∫—Ä—É–≥ —Å–µ–≥–º–µ–Ω—Ç–∞"""
        seg_start = segment["start"]
        seg_end = segment["end"]
        
        # –†–∞—Å—à–∏—Ä—è–µ–º —Å—Ü–µ–Ω—É –Ω–∞ 15 —Å–µ–∫—É–Ω–¥ –≤ –∫–∞–∂–¥—É—é —Å—Ç–æ—Ä–æ–Ω—É
        scene_start = max(0, seg_start - 15)
        scene_end = seg_end + 15
        duration = scene_end - scene_start
        
        return {
            "start_timecode": self.seconds_to_timecode(scene_start),
            "end_timecode": self.seconds_to_timecode(scene_end),
            "start_seconds": scene_start,
            "end_seconds": scene_end,
            "duration": duration,
            "scene_id": 1
        }
    
    def create_single_clip(self, video_path, segment, scene, clip_number, output_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∫–ª–∏–ø–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏"""
        
        output_path = os.path.join(output_dir, f"clip_{clip_number:03d}.mp4")
        ass_path = os.path.join(output_dir, f"clip_{clip_number:03d}.ass")
        
        # –°–æ–∑–¥–∞–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã
        self.create_subtitles(segment, scene, ass_path)
        
        # –ö–æ–º–∞–Ω–¥–∞ FFmpeg —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ ASS —Ñ–∞–π–ª—É
        ass_path_fixed = ass_path.replace('\\', '/')  # –î–ª—è Windows
        
        cmd = [
            "ffmpeg", "-y", "-loglevel", "warning",  # –ë–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ—à–∏–±–∫–∞—Ö
            "-i", video_path,
            "-ss", str(scene["start_seconds"]),
            "-t", str(scene["duration"]),
            "-vf", (
                f"scale=1080:1920:force_original_aspect_ratio=decrease,"
                f"pad=1080:1920:(ow-iw)/2:(oh-ih)/2:color=black,"
                f"ass='{ass_path_fixed}'"  # –ü—É—Ç—å –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            ),
            "-c:a", "aac", "-b:a", "128k",
            "-c:v", "libx264", "-preset", "fast", "-crf", "25",
            output_path
        ]
        
        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=120
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ—à–∏–±–∫–∞ —Å ASS —Ñ–∞–π–ª–æ–º
            if result.returncode != 0 and "libass" in result.stderr:
                print(f"{Fore.YELLOW}‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏, —Å–æ–∑–¥–∞—é –±–µ–∑ –Ω–∏—Ö...")
                # –°–æ–∑–¥–∞–µ–º –≤–∏–¥–µ–æ –±–µ–∑ —Å—É–±—Ç–∏—Ç—Ä–æ–≤
                cmd_no_subs = [
                    "ffmpeg", "-y", "-loglevel", "error",
                    "-i", video_path,
                    "-ss", str(scene["start_seconds"]),
                    "-t", str(scene["duration"]),
                    "-vf", (
                        "scale=1080:1920:force_original_aspect_ratio=decrease,"
                        "pad=1080:1920:(ow-iw)/2:(oh-ih)/2:color=black"
                    ),
                    "-c:a", "aac", "-b:a", "128k",
                    "-c:v", "libx264", "-preset", "fast", "-crf", "25",
                    output_path
                ]
                result = subprocess.run(cmd_no_subs, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0 and os.path.exists(output_path):
                return {
                    "path": output_path,
                    "segment": segment,
                    "scene": scene,
                    "ai_score": segment.get("ai_score", 0),
                    "duration": scene["duration"]
                }
            else:
                print(f"{Fore.RED}‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∏–ø–∞ {clip_number}")
                if result.stderr:
                    print(f"–û—à–∏–±–∫–∞ FFmpeg: {result.stderr[:200]}")
                return None
                
        except Exception as e:
            print(f"{Fore.RED}üí• –û—à–∏–±–∫–∞: {e}")
            return None
    
    def create_subtitles(self, segment, scene, ass_path):
        """–°–æ–∑–¥–∞–Ω–∏–µ ASS —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –¥–ª—è –∫–ª–∏–ø–∞"""
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ ASS —Ñ–∞–π–ª–∞
        header = """[Script Info]
Title: Enhanced AI Clip
ScriptType: v4.00+
PlayResX: 1080
PlayResY: 1920

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial Black,56,&H00FFFFFF,&H00FFFFFF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,4,2,2,30,30,120,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
        
        lines = [header]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞
        if segment["words"]:
            # –ö–∞—Ä–∞–æ–∫–µ —Å—É–±—Ç–∏—Ç—Ä—ã (–ø–æ —Å–ª–æ–≤–∞–º)
            seg_start_in_clip = max(0, segment["start"] - scene["start_seconds"])
            karaoke_text = ""
            
            for word_data in segment["words"]:
                if (word_data["start"] >= scene["start_seconds"] and 
                    word_data["end"] <= scene["end_seconds"]):
                    
                    duration_cs = max(int((word_data["end"] - word_data["start"]) * 100), 10)
                    word_text = word_data["text"]
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏
                    emoji_map = self.config.get("emoji_map", {})
                    for keyword, emoji in emoji_map.items():
                        if keyword.lower() in word_text.lower():
                            word_text = f"{word_text} {emoji}"
                            break
                    
                    karaoke_text += f"{{\\kf{duration_cs}}}{word_text} "
            
            if karaoke_text.strip():
                seg_end_in_clip = min(segment["end"] - scene["start_seconds"], scene["duration"])
                
                lines.append(
                    f"Dialogue: 0,{self.format_time(seg_start_in_clip)},{self.format_time(seg_end_in_clip)},"
                    f"Default,,0,0,0,,{karaoke_text.strip()}"
                )
        else:
            # –ü—Ä–æ—Å—Ç—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã (–≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ä–∞–∑—É)
            seg_start_in_clip = max(0, segment["start"] - scene["start_seconds"])
            seg_end_in_clip = min(segment["end"] - scene["start_seconds"], scene["duration"])
            
            subtitle_text = segment["text"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏
            emoji_map = self.config.get("emoji_map", {})
            for keyword, emoji in emoji_map.items():
                if keyword.lower() in subtitle_text.lower():
                    subtitle_text = f"{subtitle_text} {emoji}"
            
            lines.append(
                f"Dialogue: 0,{self.format_time(seg_start_in_clip)},{self.format_time(seg_end_in_clip)},"
                f"Default,,0,0,0,,{subtitle_text}"
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(ass_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
    
    def format_time(self, seconds):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è ASS"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        centiseconds = int((seconds * 100) % 100)
        return f"{hours}:{minutes:02d}:{secs:02d}.{centiseconds:02d}"
    
    def seconds_to_timecode(self, seconds):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–µ–∫—É–Ω–¥ –≤ —Ç–∞–π–º–∫–æ–¥"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

class FixedAIVideoSorter:
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è AI —Å–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞"""
    
    def __init__(self, config_file="config.json"):
        self.analyzer = VideoAnalyzer(config_file)
        self.config = self.analyzer.config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.transcriber = EnhancedTranscriber(self.config["whisper_model"])
        self.ai_analyzer = AIContentAnalyzer()
        self.scene_detector = ImprovedSceneDetector(self.config["scene_threshold"])
        self.video_processor = SimpleVideoProcessor(self.config)
        
        print(f"{Fore.GREEN}{Style.BRIGHT}üöÄ Enhanced AI Video Sorter (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è) –≥–æ—Ç–æ–≤!")
        
    def process_video(self, video_path: str, output_dir: str = "enhanced_clips"):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        
        if not os.path.exists(video_path):
            print(f"{Fore.RED}‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
            return None
        
        print(f"{Fore.CYAN}{Style.BRIGHT}üé¨ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É: {video_path}")
        print("=" * 60)
        
        try:
            # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
            print(f"{Back.BLUE}{Fore.WHITE} –≠–¢–ê–ü 1: –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø {Style.RESET_ALL}")
            transcript_data = self.transcriber.transcribe_with_analysis(video_path)
            
            if not transcript_data["segments"]:
                print(f"{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é")
                return None
            
            # 2. AI –∞–Ω–∞–ª–∏–∑
            print(f"\n{Back.MAGENTA}{Fore.WHITE} –≠–¢–ê–ü 2: –ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ù–¢–ê {Style.RESET_ALL}")
            analyzed_segments = self.ai_analyzer.analyze_segments(transcript_data)
            
            # 3. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ü–µ–Ω
            print(f"\n{Back.GREEN}{Fore.WHITE} –≠–¢–ê–ü 3: –ê–ù–ê–õ–ò–ó –°–¶–ï–ù {Style.RESET_ALL}")
            scenes = self.scene_detector.detect_scenes(video_path, transcript_data)
            
            if not scenes:
                print(f"{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ü–µ–Ω—ã")
                return None
            
            # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–ø–æ–≤
            print(f"\n{Back.RED}{Fore.WHITE} –≠–¢–ê–ü 4: –°–û–ó–î–ê–ù–ò–ï –ö–õ–ò–ü–û–í {Style.RESET_ALL}")
            created_clips = self.video_processor.create_clips(
                video_path, analyzed_segments, scenes, output_dir
            )
            
            # 5. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            self.print_results(created_clips, output_dir)
            
            return created_clips
            
        except Exception as e:
            print(f"{Fore.RED}üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def print_results(self, clips, output_dir):
        """–ü–µ—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}=" * 60)
        print(f"üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
        print(f"üé¨ –°–æ–∑–¥–∞–Ω–æ –∫–ª–∏–ø–æ–≤: {len(clips)}")
        
        if clips:
            print(f"\n{Fore.YELLOW}üìä –¢–û–ü –ö–õ–ò–ü–´ –ü–û –û–¶–ï–ù–ö–ï:")
            for i, clip in enumerate(clips[:5], 1):
                score = clip["ai_score"]
                duration = clip["duration"]
                print(f"  {i}. {os.path.basename(clip['path'])} "
                      f"(–û—Ü–µ–Ω–∫–∞: {score:.2f}, –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f}—Å)")
        
        print(f"\n{Fore.CYAN}üí° –ö–ª–∏–ø—ã –≥–æ—Ç–æ–≤—ã –∫ –ø—Ä–æ—Å–º–æ—Ç—Ä—É!")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print(f"{Fore.MAGENTA}{Style.BRIGHT}")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë     ü§ñ ENHANCED AI VIDEO SORTER              ‚ïë")
    print("‚ïë         –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è v1.1.            ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print(Style.RESET_ALL)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        video_file = sys.argv[1]
    else:
        video_file = input(f"{Fore.CYAN}üìπ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—É: {Style.RESET_ALL}")
    
    if not video_file or not os.path.exists(video_file):
        print(f"{Fore.RED}‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω!")
        return
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞
    sorter = FixedAIVideoSorter()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
    clips = sorter.process_video(video_file)
    
    if clips and len(clips) > 0:
        print(f"\n{Fore.GREEN}‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–∏—Ç–µ –≤–∞—à–∏ –∫–ª–∏–ø—ã –≤ –ø–∞–ø–∫–µ 'enhanced_clips'")
        print(f"{Fore.CYAN}üé¨ –°–æ–∑–¥–∞–Ω–æ {len(clips)} –∫–ª–∏–ø–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    else:
        print(f"\n{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–ø—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ.")

if __name__ == "__main__":
    main()