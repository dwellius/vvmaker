#!/usr/bin/env python3
"""
Enhanced AI Video Sorter - –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏—Ä—É—Å–Ω—ã—Ö –∫–ª–∏–ø–æ–≤
–ü–æ—Ö–æ–∂–µ –Ω–∞ OpusClip, –Ω–æ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º
"""

import os
import json
import random
import subprocess
import sys
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import time
import math

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–∫–µ—Ç—ã
def install_requirements():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"""
    packages = [
        'faster-whisper',
        'scenedetect[opencv]',
        'openai',
        'nltk',
        'textstat',
        'colorama',
        'tqdm'
    ]
    
    for package in packages:
        try:
            __import__(package.replace('-', '_').split('[')[0])
        except ImportError:
            print(f"–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–∫–µ—Ç—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
install_requirements()

from faster_whisper import WhisperModel
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
import openai
import nltk
import textstat
from colorama import init, Fore, Style, Back
from tqdm import tqdm

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–≤–µ—Ç–∞ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
init(autoreset=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ OpenAI –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
openai.api_key = "sk-emergent-fDbDaD9E72a58Ab4bE"
openai.base_url = "https://api.emergent.com"

class VideoAnalyzer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–¥–µ–æ —Å AI –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    
    def __init__(self, config_file="config.json"):
        self.config = self.load_config(config_file)
        self.setup_nltk()
        
    def setup_nltk(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            print("–ó–∞–≥—Ä—É–∂–∞—é —è–∑—ã–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('vader_lexicon', quiet=True)
    
    def load_config(self, config_file):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        default_config = {
            "whisper_model": "small",
            "scene_threshold": 30.0,
            "min_clip_duration": 15,
            "max_clip_duration": 60,
            "min_words": 8,
            "output_resolution": "1080x1920",
            "subtitle_styles": {
                "modern": {
                    "font": "Arial Black",
                    "size": 56,
                    "color": "&H00FFFFFF",
                    "outline": 4,
                    "shadow": 2
                },
                "neon": {
                    "font": "Impact", 
                    "size": 64,
                    "color": "&H0000FFFF",
                    "outline": 6,
                    "shadow": 3
                },
                "elegant": {
                    "font": "Times New Roman",
                    "size": 52,
                    "color": "&H00F0F0F0", 
                    "outline": 3,
                    "shadow": 1
                }
            },
            "emoji_map": {
                "—Å–º–µ—à–Ω–æ": "üòÇ", "—é–º–æ—Ä": "üòÑ", "—Ä–∂–∞–∫–∞": "ü§£",
                "–æ—à–∏–±–∫–∞": "‚ùå", "–ø—Ä–æ–±–ª–µ–º–∞": "‚ö†Ô∏è", "–±–∞–≥": "üêõ",
                "–≤–∞–∂–Ω–æ": "‚≠ê", "–≤–Ω–∏–º–∞–Ω–∏–µ": "üì¢", "–∫–ª—é—á–µ–≤–æ–µ": "üîë",
                "–∏–¥–µ—è": "üí°", "–º—ã—Å–ª—å": "üß†", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è": "üöÄ",
                "—Å–æ–≤–µ—Ç": "üìå", "–ª–∞–π—Ñ—Ö–∞–∫": "üíØ", "—Å–µ–∫—Ä–µ—Ç": "üîê",
                "–¥–µ–Ω—å–≥–∏": "üí∞", "–±–∏–∑–Ω–µ—Å": "üìà", "—É—Å–ø–µ—Ö": "üéØ",
                "–ª—é–±–æ–≤—å": "‚ù§Ô∏è", "—Å–µ–º—å—è": "üë®‚Äçüë©‚Äçüëß‚Äçüë¶", "–¥—Ä—É–∑—å—è": "üë•",
                "—Å–ø–æ—Ä—Ç": "‚öΩ", "–∑–¥–æ—Ä–æ–≤—å–µ": "üí™", "—Ñ–∏—Ç–Ω–µ—Å": "üèãÔ∏è",
                "–µ–¥–∞": "üçï", "–≥–æ—Ç–æ–≤–∫–∞": "üë®‚Äçüç≥", "—Ä–µ—Ü–µ–ø—Ç": "üìù",
                "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ": "‚úàÔ∏è", "–ø—Ä–∏–∫–ª—é—á–µ–Ω–∏–µ": "üó∫Ô∏è", "–æ—Ç–¥—ã—Ö": "üèñÔ∏è",
                "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏": "üíª", "AI": "ü§ñ", "–±—É–¥—É—â–µ–µ": "üîÆ",
                "–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ": "üìö", "—É—á–µ–±–∞": "üéì", "–∑–Ω–∞–Ω–∏—è": "üìñ"
            },
            "keywords": {
                "high_priority": ["–≤–∞–∂–Ω–æ", "—Å–µ–∫—Ä–µ—Ç", "–æ—à–∏–±–∫–∞", "—Å–æ–≤–µ—Ç", "–ª–∞–π—Ñ—Ö–∞–∫", "–≤–Ω–∏–º–∞–Ω–∏–µ"],
                "medium_priority": ["–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—Å–º–µ—à–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ"],
                "emotions": ["—Ä–∞–¥–æ—Å—Ç—å", "–≥—Ä—É—Å—Ç—å", "–∑–ª–æ—Å—Ç—å", "—É–¥–∏–≤–ª–µ–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö"],
                "engagement": ["–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å", "–ª–∞–π–∫", "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π", "–ø–æ–¥–µ–ª–∏—Ç–µ—Å—å"]
            }
        }
        
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                default_config.update(user_config)
        else:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
                
        return default_config

class EnhancedTranscriber:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º —ç–º–æ—Ü–∏–π"""
    
    def __init__(self, model_size="small"):
        self.model = WhisperModel(model_size, device="cpu")
        print(f"{Fore.CYAN}üé§ Whisper –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_size}")
        
    def transcribe_with_analysis(self, video_path: str) -> Dict:
        """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        print(f"{Fore.YELLOW}üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –≤–∏–¥–µ–æ: {video_path}")
        
        segments, info = self.model.transcribe(
            video_path, 
            word_timestamps=True,
            language="ru"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        )
        
        transcript = []
        full_text = ""
        
        for seg in tqdm(segments, desc="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å–µ–≥–º–µ–Ω—Ç—ã"):
            words = []
            if seg.words:
                for w in seg.words:
                    words.append({
                        "start": w.start,
                        "end": w.end,
                        "text": w.word.strip(),
                        "confidence": getattr(w, 'probability', 1.0)
                    })
            
            segment_text = seg.text.strip()
            full_text += segment_text + " "
            
            # –ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–∞
            segment_data = {
                "start": seg.start,
                "end": seg.end,
                "text": segment_text,
                "words": words,
                "word_count": len(segment_text.split()),
                "reading_level": textstat.flesch_reading_ease(segment_text) if segment_text else 0,
                "excitement_score": self.calculate_excitement(segment_text),
                "has_question": "?" in segment_text,
                "has_exclamation": "!" in segment_text
            }
            
            transcript.append(segment_data)
        
        return {
            "segments": transcript,
            "full_text": full_text.strip(),
            "total_duration": info.duration,
            "language": info.language,
            "language_probability": info.language_probability
        }
    
    def calculate_excitement(self, text: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å '–≤–æ–∑–±—É–∂–¥–µ–Ω–∏—è' —Ç–µ–∫—Å—Ç–∞"""
        excitement_words = [
            "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "—Ñ–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏", "–ø–æ—Ç—Ä—è—Å–∞—é—â–µ",
            "—à–æ–∫", "–≤–∞—É", "–æ—Ñ–∏–≥–µ—Ç—å", "–∫—Ä—É—Ç–æ", "—Å—É–ø–µ—Ä", "–º–µ–≥–∞"
        ]
        
        score = 0
        words = text.lower().split()
        
        for word in words:
            if word in excitement_words:
                score += 2
            if word.endswith("!"):
                score += 1
                
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        return min(score / max(len(words), 1), 1.0)

class AIContentAnalyzer:
    """AI –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª—É—á—à–∏—Ö –º–æ–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.client = openai.OpenAI(
            api_key="sk-emergent-fDbDaD9E72a58Ab4bE",
            base_url="https://api.emergent.com"
        )
    
    def analyze_segments(self, transcript_data: Dict) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é AI"""
        segments = transcript_data["segments"]
        
        print(f"{Fore.MAGENTA}ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é AI...")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ 5-10 —Å–µ–≥–º–µ–Ω—Ç–æ–≤)
        analyzed_segments = []
        
        for i in range(0, len(segments), 8):
            batch = segments[i:i+8]
            batch_text = "\n".join([f"{j+1}. {seg['text']}" for j, seg in enumerate(batch)])
            
            if len(batch_text.strip()) < 50:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                for seg in batch:
                    seg["ai_score"] = 0.3
                    seg["ai_reason"] = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
                analyzed_segments.extend(batch)
                continue
            
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π AI –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é –≤–∏—Ä—É—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞. 
                            –û—Ü–µ–Ω–∏ –∫–∞–∂–¥—ã–π —Å–µ–≥–º–µ–Ω—Ç –æ—Ç 0.0 –¥–æ 1.0 –ø–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—É —Å—Ç–∞—Ç—å –≤–∏—Ä—É—Å–Ω—ã–º.
                            –£—á–∏—Ç—ã–≤–∞–π: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å, –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç—å, —é–º–æ—Ä.
                            –û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
                            {"segments": [{"index": 1, "score": 0.8, "reason": "–ø—Ä–∏—á–∏–Ω–∞"}]}"""
                        },
                        {
                            "role": "user",
                            "content": f"–û—Ü–µ–Ω–∏ —ç—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç—ã:\n{batch_text}"
                        }
                    ],
                    temperature=0.3,
                    max_tokens=500
                )
                
                result = json.loads(response.choices[0].message.content)
                
                for item in result.get("segments", []):
                    idx = item.get("index", 1) - 1
                    if idx < len(batch):
                        batch[idx]["ai_score"] = item.get("score", 0.5)
                        batch[idx]["ai_reason"] = item.get("reason", "AI –∞–Ω–∞–ª–∏–∑")
                        
            except Exception as e:
                print(f"{Fore.RED}‚ö†Ô∏è –û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {e}")
                # Fallback –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI
                for seg in batch:
                    score = self.fallback_analysis(seg)
                    seg["ai_score"] = score
                    seg["ai_reason"] = "–õ–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"
            
            analyzed_segments.extend(batch)
            time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        return analyzed_segments
    
    def fallback_analysis(self, segment: Dict) -> float:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI"""
        score = 0.0
        text = segment["text"].lower()
        
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        high_value_words = ["—Å–µ–∫—Ä–µ—Ç", "–æ—à–∏–±–∫–∞", "—Å–æ–≤–µ—Ç", "–≤–∞–∂–Ω–æ", "–ª–∞–π—Ñ—Ö–∞–∫"]
        emotion_words = ["—Å–º–µ—à–Ω–æ", "—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "—à–æ–∫", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ"]
        
        for word in high_value_words:
            if word in text:
                score += 0.3
                
        for word in emotion_words:
            if word in text:
                score += 0.2
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–ª–ª—ã –∑–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        if "?" in text:
            score += 0.1
        if "!" in text:
            score += 0.15
            
        # –£—á–∏—Ç—ã–≤–∞–µ–º excitement_score
        score += segment.get("excitement_score", 0) * 0.2
        
        return min(score, 1.0)

class AdvancedSceneDetector:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å—Ü–µ–Ω"""
    
    def __init__(self, threshold=30.0):
        self.threshold = threshold
        
    def detect_scenes(self, video_path: str) -> List[Tuple]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ü–µ–Ω —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        print(f"{Fore.GREEN}üé¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ü–µ–Ω—ã –≤ –≤–∏–¥–µ–æ...")
        
        video_manager = VideoManager([video_path])
        scene_manager = SceneManager()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
        scene_manager.add_detector(ContentDetector(threshold=self.threshold))
        
        video_manager.start()
        scene_manager.detect_scenes(frame_source=video_manager)
        scene_list = scene_manager.get_scene_list()
        
        scenes = []
        for i, (start, end) in enumerate(scene_list):
            duration = (end - start).get_seconds()
            scenes.append({
                "start_timecode": start.get_timecode(),
                "end_timecode": end.get_timecode(), 
                "start_seconds": start.get_seconds(),
                "end_seconds": end.get_seconds(),
                "duration": duration,
                "scene_id": i + 1
            })
            
        print(f"{Fore.GREEN}‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ü–µ–Ω: {len(scenes)}")
        return scenes

class EnhancedSubtitleGenerator:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏"""
    
    def __init__(self, config):
        self.config = config
        self.styles = config["subtitle_styles"]
        
    def generate_ass_subtitles(self, transcript, start_sec, end_sec, output_path, style_name="modern"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ASS —Å—É–±—Ç–∏—Ç—Ä–æ–≤ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å—Ç–∏–ª–µ–º"""
        
        style = self.styles.get(style_name, self.styles["modern"])
        
        header = f"""[Script Info]
Title: Enhanced AI Clip
ScriptType: v4.00+
PlayResX: 1080
PlayResY: 1920
YCbCr Matrix: TV.709

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,{style['font']},{style['size']},{style['color']}&HFFFFFF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,{style['outline']},{style['shadow']},2,30,30,100,1
Style: Highlight,{style['font']},{style['size'] + 8},&H0000FFFF,&H00FFFFFF,&H00000000,&H80000000,-1,0,0,0,110,110,0,0,1,{style['outline'] + 1},{style['shadow'] + 1},2,30,30,80,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
        
        lines = [header]
        emoji_map = self.config["emoji_map"]
        
        for segment in transcript:
            if (segment["start"] >= start_sec and segment["end"] <= end_sec and 
                segment.get("words")):
                
                seg_start = segment["start"] - start_sec
                seg_end = segment["end"] - start_sec
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ AI –æ—Ü–µ–Ω–∫–∏
                use_highlight = segment.get("ai_score", 0) > 0.7
                subtitle_style = "Highlight" if use_highlight else "Default"
                
                # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä–∞–æ–∫–µ —ç—Ñ—Ñ–µ–∫—Ç
                karaoke_text = self.create_karaoke_effect(segment["words"], emoji_map)
                
                if karaoke_text.strip():
                    lines.append(
                        f"Dialogue: 0,{self.format_time(seg_start)},{self.format_time(seg_end)},"
                        f"{subtitle_style},,0,0,0,,{karaoke_text}"
                    )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
    
    def create_karaoke_effect(self, words, emoji_map):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä–∞–æ–∫–µ —ç—Ñ—Ñ–µ–∫—Ç–∞ —Å —ç–º–æ–¥–∑–∏"""
        karaoke_text = ""
        
        for word_data in words:
            duration_cs = max(int((word_data["end"] - word_data["start"]) * 100), 10)
            word_text = word_data["text"]
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏
            for keyword, emoji in emoji_map.items():
                if keyword.lower() in word_text.lower():
                    word_text = f"{word_text} {emoji}"
                    break
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
            confidence = word_data.get("confidence", 1.0)
            if confidence < 0.7:  # –ù–µ—É–≤–µ—Ä–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
                word_text = f"{{\\alpha&H80&}}{word_text}{{\\alpha&H00&}}"
            
            karaoke_text += f"{{\\kf{duration_cs}}}{word_text} "
        
        return karaoke_text.strip()
    
    def format_time(self, seconds):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è ASS"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        centiseconds = int((seconds * 100) % 100)
        return f"{hours}:{minutes:02d}:{secs:02d}.{centiseconds:02d}"

class AdvancedVideoProcessor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤–∏–¥–µ–æ —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º–∏ —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
    
    def __init__(self, config):
        self.config = config
        
    def create_enhanced_clips(self, video_path, analyzed_segments, scenes, output_dir="enhanced_clips"):
        """–°–æ–∑–¥–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∫–ª–∏–ø–æ–≤"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"{Fore.CYAN}üé• –°–æ–∑–¥–∞—é —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–ª–∏–ø—ã...")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ AI –æ—Ü–µ–Ω–∫–µ
        good_segments = [s for s in analyzed_segments if s.get("ai_score", 0) > 0.4]
        good_segments.sort(key=lambda x: x.get("ai_score", 0), reverse=True)
        
        # –ë–µ—Ä–µ–º —Ç–æ–ø —Å–µ–≥–º–µ–Ω—Ç—ã
        top_segments = good_segments[:min(10, len(good_segments))]
        
        created_clips = []
        
        for i, segment in enumerate(tqdm(top_segments, desc="–°–æ–∑–¥–∞—é –∫–ª–∏–ø—ã")):
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Ü–µ–Ω—É
            scene = self.find_best_scene(segment, scenes)
            if not scene:
                continue
                
            clip_info = self.create_single_clip(
                video_path, segment, scene, i + 1, output_dir
            )
            
            if clip_info:
                created_clips.append(clip_info)
        
        return created_clips
    
    def find_best_scene(self, segment, scenes):
        """–ù–∞–π—Ç–∏ –ª—É—á—à—É—é —Å—Ü–µ–Ω—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        seg_start, seg_end = segment["start"], segment["end"]
        
        for scene in scenes:
            if (scene["start_seconds"] <= seg_start and 
                scene["end_seconds"] >= seg_end and
                scene["duration"] >= self.config["min_clip_duration"] and
                scene["duration"] <= self.config["max_clip_duration"]):
                return scene
        
        return None
    
    def create_single_clip(self, video_path, segment, scene, clip_number, output_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∫–ª–∏–ø–∞ —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
        
        output_path = os.path.join(output_dir, f"clip_{clip_number:03d}.mp4")
        ass_path = os.path.join(output_dir, f"clip_{clip_number:03d}.ass")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã
        subtitle_gen = EnhancedSubtitleGenerator(self.config)
        style = self.choose_subtitle_style(segment)
        
        subtitle_gen.generate_ass_subtitles(
            [segment], scene["start_seconds"], scene["end_seconds"],
            ass_path, style
        )
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–¥–µ–æ —ç—Ñ—Ñ–µ–∫—Ç—ã
        effects = self.generate_video_effects(segment, scene)
        
        # –ö–æ–º–∞–Ω–¥–∞ FFmpeg
        cmd = self.build_ffmpeg_command(
            video_path, scene, ass_path, output_path, effects
        )
        
        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, timeout=300
            )
            
            if result.returncode == 0:
                return {
                    "path": output_path,
                    "segment": segment,
                    "scene": scene,
                    "ai_score": segment.get("ai_score", 0),
                    "duration": scene["duration"]
                }
            else:
                print(f"{Fore.RED}‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–ª–∏–ø–∞ {clip_number}: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            print(f"{Fore.RED}‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–ª–∏–ø–∞ {clip_number}")
            return None
        except Exception as e:
            print(f"{Fore.RED}üí• –û—à–∏–±–∫–∞: {e}")
            return None
    
    def choose_subtitle_style(self, segment):
        """–í—ã–±–æ—Ä —Å—Ç–∏–ª—è —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        ai_score = segment.get("ai_score", 0)
        
        if ai_score > 0.8:
            return "neon"  # –î–ª—è –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        elif ai_score > 0.6:
            return "modern"  # –î–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        else:
            return "elegant"  # –î–ª—è –æ–±—ã—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    
    def generate_video_effects(self, segment, scene):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–¥–µ–æ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤"""
        effects = {
            "zoom": random.choice([1.0, 1.05, 1.1, 1.15]),
            "pan": random.choice([None, "left", "right", "up", "down"]),
            "color_grade": random.choice([None, "warm", "cool", "vibrant"]),
            "transition": random.choice(["fade", "slide", "zoom_in"]),
            "jump_cut": random.random() > 0.7,  # 30% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            "speed_ramp": segment.get("ai_score", 0) > 0.7  # –î–ª—è –≤–∞–∂–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        }
        
        return effects
    
    def build_ffmpeg_command(self, video_path, scene, ass_path, output_path, effects):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã FFmpeg —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏"""
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        start_time = scene["start_timecode"]
        duration = scene["duration"]
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–ª—å—Ç—Ä
        video_filters = []
        
        # 1. –ë–∞–∑–æ–≤–æ–µ –∫–∞–¥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
        video_filters.append("[0:v]scale=1080:1920,boxblur=20:5[bg]")
        
        # 2. –ü–µ—Ä–µ–¥–Ω–∏–π –ø–ª–∞–Ω
        crop_scale = (
            "[0:v]crop=in_h*9/16:in_h:(in_w-in_h*9/16)/2:0,"
            "scale=1080:1920:force_original_aspect_ratio=decrease,"
            "pad=1080:1920:(ow-iw)/2:(oh-ih)/2"
        )
        
        # 3. –î–æ–±–∞–≤–ª—è–µ–º zoom —ç—Ñ—Ñ–µ–∫—Ç
        if effects["zoom"] > 1.0:
            zoom_filter = f",zoompan=z='min(zoom+0.002,{effects['zoom']})':d=1:s=1080x1920"
            crop_scale += zoom_filter
        
        # 4. –¶–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è
        if effects["color_grade"]:
            if effects["color_grade"] == "warm":
                crop_scale += ",eq=temperature=200:brightness=0.05"
            elif effects["color_grade"] == "cool":
                crop_scale += ",eq=temperature=-200:brightness=0.05"
            elif effects["color_grade"] == "vibrant":
                crop_scale += ",eq=saturation=1.2:contrast=1.1"
        
        crop_scale += "[fg]"
        video_filters.append(crop_scale)
        
        # 5. –ö–æ–º–ø–æ–∑–∏—Ç–∏–Ω–≥
        composite = f"[bg][fg]overlay=(W-w)/2:(H-h)/2"
        
        # 6. Jump cut —ç—Ñ—Ñ–µ–∫—Ç
        if effects["jump_cut"]:
            cut_time = random.uniform(duration * 0.3, duration * 0.7)
            cut_duration = 0.3
            composite += f",select='not(between(t,{cut_time},{cut_time + cut_duration}))',setpts=N/FRAME_RATE/TB"
        
        # 7. –î–æ–±–∞–≤–ª—è–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã
        composite += f",ass={ass_path}"
        
        video_filters.append(composite)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–∞–Ω–¥—É
        cmd = [
            "ffmpeg", "-y", "-loglevel", "error",
            "-i", video_path,
            "-ss", start_time,
            "-t", str(duration),
            "-vf", ";".join(video_filters),
            "-c:a", "aac", "-b:a", "128k",
            "-c:v", "libx264", "-preset", "medium", "-crf", "23",
            "-movflags", "+faststart",
            output_path
        ]
        
        return cmd

class EnhancedAIVideoSorter:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ AI —Å–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞"""
    
    def __init__(self, config_file="config.json"):
        self.analyzer = VideoAnalyzer(config_file)
        self.config = self.analyzer.config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.transcriber = EnhancedTranscriber(self.config["whisper_model"])
        self.ai_analyzer = AIContentAnalyzer()
        self.scene_detector = AdvancedSceneDetector(self.config["scene_threshold"])
        self.video_processor = AdvancedVideoProcessor(self.config)
        
        print(f"{Fore.GREEN}{Style.BRIGHT}üöÄ Enhanced AI Video Sorter –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        
    def process_video(self, video_path: str, output_dir: str = "enhanced_clips"):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ"""
        
        if not os.path.exists(video_path):
            print(f"{Fore.RED}‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {video_path}")
            return None
        
        print(f"{Fore.CYAN}{Style.BRIGHT}üé¨ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É: {video_path}")
        print("=" * 60)
        
        # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        print(f"{Back.BLUE}{Fore.WHITE} –≠–¢–ê–ü 1: –¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø {Style.RESET_ALL}")
        transcript_data = self.transcriber.transcribe_with_analysis(video_path)
        
        # 2. AI –∞–Ω–∞–ª–∏–∑
        print(f"\n{Back.MAGENTA}{Fore.WHITE} –≠–¢–ê–ü 2: AI –ê–ù–ê–õ–ò–ó –ö–û–ù–¢–ï–ù–¢–ê {Style.RESET_ALL}")
        analyzed_segments = self.ai_analyzer.analyze_segments(transcript_data)
        
        # 3. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ü–µ–Ω
        print(f"\n{Back.GREEN}{Fore.WHITE} –≠–¢–ê–ü 3: –ê–ù–ê–õ–ò–ó –°–¶–ï–ù {Style.RESET_ALL}")
        scenes = self.scene_detector.detect_scenes(video_path)
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–ø–æ–≤
        print(f"\n{Back.RED}{Fore.WHITE} –≠–¢–ê–ü 4: –°–û–ó–î–ê–ù–ò–ï –ö–õ–ò–ü–û–í {Style.RESET_ALL}")
        created_clips = self.video_processor.create_enhanced_clips(
            video_path, analyzed_segments, scenes, output_dir
        )
        
        # 5. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        self.print_results(created_clips, output_dir)
        
        return created_clips
    
    def print_results(self, clips, output_dir):
        """–ü–µ—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print(f"\n{Fore.GREEN}{Style.BRIGHT}=" * 60)
        print(f"üéâ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
        print(f"üé¨ –°–æ–∑–¥–∞–Ω–æ –∫–ª–∏–ø–æ–≤: {len(clips)}")
        
        if clips:
            print(f"\n{Fore.YELLOW}üìä –¢–û–ü –ö–õ–ò–ü–´ –ü–û AI –û–¶–ï–ù–ö–ï:")
            for i, clip in enumerate(clips[:5], 1):
                score = clip["ai_score"]
                duration = clip["duration"]
                print(f"  {i}. {os.path.basename(clip['path'])} "
                      f"(–û—Ü–µ–Ω–∫–∞: {score:.2f}, –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f}—Å)")
        
        print(f"\n{Fore.CYAN}üí° –°–æ–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–ª–∏–ø—ã –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ª—É—á—à–∏–µ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏!")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print(f"{Fore.MAGENTA}{Style.BRIGHT}")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë        ü§ñ ENHANCED AI VIDEO SORTER 2.0      ‚ïë")
    print("‚ïë              Powered by AI                   ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print(Style.RESET_ALL)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        video_file = sys.argv[1]
    else:
        video_file = input(f"{Fore.CYAN}üìπ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ —Ñ–∞–π–ª—É: {Style.RESET_ALL}")
    
    if not video_file or not os.path.exists(video_file):
        print(f"{Fore.RED}‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω!")
        return
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞
    sorter = EnhancedAIVideoSorter()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
    clips = sorter.process_video(video_file)
    
    if clips:
        print(f"\n{Fore.GREEN}‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–∏—Ç–µ –≤–∞—à–∏ –∫–ª–∏–ø—ã –≤ –ø–∞–ø–∫–µ 'enhanced_clips'")
    else:
        print(f"\n{Fore.RED}‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫–ª–∏–ø—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ.")


if __name__ == "__main__":
    main()